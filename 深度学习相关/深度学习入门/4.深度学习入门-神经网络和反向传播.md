# 深度学习入门

**Deep Learning Beginning**

---

---

## 神经网络和反向传播

### 神经元简介

神经元是神经网络的基本构建单元，模拟了生物神经元的工作原理。每个神经元接收来自其他神经元或输入层的数据，通过加权求和并应用激活函数，生成输出信号。

神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是**阶跃函数**，而当我们说神经元时，激活函数往往选择有以下选择：

- **线性函数（Linear Function）**：输出等于输入。
- **Sigmoid函数**：输出值在0和1之间，适用于二分类问题。
- **ReLU函数（Rectified Linear Unit）**：当输入大于0时，输出等于输入；否则，输出为0。
- **Leaky ReLU**：对ReLU的改进，Leaky ReLU函数通过把非常小的线性分量给予负输入，调整负值的零梯度问题。
- **Tanh函数**：输出值在-1和1之间，适用于需要输出为负值的场景。

<img src="./assets/2.jpg" alt="2" style="zoom:80%;" />

### 神经元输出计算

神经元的输出计算与感知器的输出计算方法相同。

设神经元的输入向量为 $\vec{x}$，权重向量为 $\vec{w}$（包含偏置项 $w_0$），并采用 sigmoid 函数作为激活函数，则神经元的输出 $y$ 可以表示为：
$$
y = \sigma(\vec{w}^T \cdot \vec{x}) \quad \tag{3.1}
$$

其中，sigmoid 函数定义为：

$$
\sigma(x) = \frac{1}{1 + e^{-x}} \tag{3.2}
$$

sigmoid 函数是一个非线性函数，其输出值域位于 $(0, 1)$ 区间内。该函数的图像展示如下。

![img](./assets/2256672-e7e64f57dc6b1c64.jpg)

sigmoid函数的导数是：

$$
\begin{align*}
\frac{d}{dx}\,\sigma(x) &= \frac{d}{dx}\left(\frac{1}{1+e^{-x}}\right) \\
&= \frac{e^{-x}}{(1+e^{-x})^{2}} \\
&= \frac{(1+e^{-x})-1}{(1+e^{-x})^{2}} \\
&= \frac{1}{1+e^{-x}} - \frac{1}{(1+e^{-x})^{2}} \\
&= \sigma(x) - \sigma(x)^{2} \\
&= \sigma(x)(1-\sigma(x))
\end{align*} \tag{3.3}
$$

可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。

### 神经网络简介

神经网络其实就是按照**特定规则**连接起来的多个**神经元**。下图展示了一个**全连接(full connected, FC)**神经网络。

![img](./assets/2256672-92111b104ce0d571.webp)

通过观察上面的图，我们可以发现它的规则包括：

- 神经元按照**层**来布局。最左边的层叫做**输入层**，负责接收输入数据；最右边的层叫**输出层**，提供网络的最终输出。输入层和输出层之间的层叫做**隐藏层**，它们对外部不可见。
- 同一层的神经元之间没有连接。
- 第N层的每个神经元和第N-1层的**所有**神经元相连（即全连接的定义），第N-1层神经元的输出就是第N层神经元的输入。
- 每个连接都有一个与之关联的**权重**。

这些规则定义了全连接神经网络的基本结构。实际上，还存在多种其他类型的神经网络结构，如卷积神经网络（CNN）和循环神经网络（RNN），它们各自具有不同的连接规则。

### 神经网络的输出计算（前向传播)

神经网络实际上就是一个输入向量$\vec{x}$到输出向量$\vec{y}$的函数。从输入层开始，一层一层地传递数据，直到最后得到预测结果。获取输出向量的过程也称为**前向传播（Forward Propagation）**。

首先，将输入向量 $\vec{x}$ 的每个元素 $x_i$ 的值赋给神经网络的输入层的对应神经元，然后根据公式 (3.1) 依次向前计算每一层的每个神经元的值，直到输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量 $\vec{y}$。

> **举例说明**
>
> 如图，这是含有一个输入层，一个输出层，一个隐藏层的神经网络。输入层包含三个节点，编号为1、2、3；隐藏层包含四个节点，编号为4、5、6、7；输出层包含两个节点，编号为8、9。在全连接神经网络中，输每个节点与上一层的所有节点相连。
>
> ![img](./assets/2256672-bfbb364740f898d1.webp)
>
> 1. **节点的输出值计算**
>
>    接下来将对节点4的输出值 $a_4$ 进行计算，举例说明如何计算节点的输出值。
>
>    首先，必须获取节点4所有上游节点（即节点1、2、3）的输出值。由于节点1、2、3位于输入层，其输出值即为输入向量 $\vec{x}$ 的元素。之后，利用公式 (3.1) 计算节点4的输出值 $a_4$：
>    $$
>    \begin{aligned}
>    a_4 & = \sigma(\vec{w}^T \cdot \vec{a}) \\
>    & = \sigma(w_{41}x_1 + w_{42}x_2 + w_{43}x_3 + w_{4b})
>    \end{aligned}
>    \tag{3.4}
>    $$
>
>    其中，$w_{4b}$ 为节点4的偏置项，图中未显示。$w_{41}, w_{42}, w_{43}$ 分别表示节点1、2、3到节点4的连接权重。权重 $w_{ji}$ 的编号规则是将目标节点编号 $j$ 置于前，源节点编号 $i$ 置于后。
>
>    同样，可以计算节点5、6、7的输出值 $a_5, a_6, a_7$。
>
> 2. **输出层的输出计算与普通节点一致。**
>
>    完成隐藏层所有节点的输出值计算后，可以继续计算输出层节点8的输出值 $y_1$：
>    $$
>    \begin{aligned}
>    y_1 & = \sigma(\vec{w}^T \cdot \vec{a}) \\
>    & = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})
>    \end{aligned}
>    \tag{3.5}
>    $$
>
>    同样，可以计算 $y_2$ 的值。
>
> 3. **向量形式**的计算表示如下：
>
>    - （加入偏置）加入偏置 $\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ 1 \end{bmatrix}$
>    - （加入偏置）输入层权重矩阵 $W_1 = \begin{bmatrix} \vec{w}_4 \\ \vec{w}_5 \\ \vec{w}_6 \\ \vec{w}_7 \end{bmatrix} = \begin{bmatrix} w_{41}, w_{42}, w_{43}, w_{4b} \\ w_{51}, w_{52}, w_{53}, w_{5b} \\ w_{61}, w_{62}, w_{63}, w_{6b} \\ w_{71}, w_{72}, w_{73}, w_{7b} \end{bmatrix}$
>    - 隐藏层输入向量 $ \vec{a} = \begin{bmatrix} a_4 \\ a_5 \\ a_6 \\ a_7 \end{bmatrix} = f(W_1 \cdot \vec{x})$ （加入偏置）$ \vec{a} = \begin{bmatrix} a_4 \\ a_5 \\ a_6 \\ a_7 \\ 1 \end{bmatrix}$
>    - （加入偏置）隐藏层权重矩阵 $W_2 = \begin{bmatrix} \vec{w}_8 \\ \vec{w}_9 \end{bmatrix} = \begin{bmatrix} w_{84}, w_{85}, w_{86}, w_{87}, w_{8b} \\ w_{94}, w_{95}, w_{96}, w_{97}, w_{9b} \end{bmatrix}$ 
>    - 输出向量为 $\vec{y} = \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = f(W_2 \cdot \vec{a})$ 

在神经网络中，每一层的计算过程是一致的。若一个神经网络含有一个输入层，一个输出层和$n-1$个隐藏层，其权重矩阵分别为 $W_n$，每个隐藏层的输出分别是 $\vec{a}_n$，神经网络的输入为 $\vec{x}$，神经网络的输入为 $\vec{y}$。

<img src="./assets/1740224603599.jpg" alt="1740224603599" style="zoom: 67%;" />

则每一层的输出向量的计算可以表示为：

$$
\begin{align*}
\vec{a}_1 & = f(W_1 \cdot \vec{x}) \\
\vec{a}_2 & = f(W_2 \cdot \vec{a}_1) \\
\vec{a}_3 & = f(W_3 \cdot \vec{a}_2) \\
& \  ... \\
\vec{a}_{n} & = f(W_{n} \cdot \vec{a}_{n-1}) \\
\vec{y} & = f(W_n \cdot \vec{a}_{n})
\end{align*}
$$

这就是神经网络输出值的计算方法。

### 神经网络的训练

我们可以说神经网络是一个**模型**，那么这些权值就是模型的**参数(Parameters)**，也就是模型要学习的东西。

然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为**超参数(Hyper-Parameters)**。

#### 计算损失（Loss）

我们假设每个训练样本为 $(\vec{x}, \vec{t})$，其中向量 $\vec{x}$ 是训练样本的特征，而 $\vec{t}$ 是样本的目标值。

![img](./assets/2256672-bfbb364740f898d1.webp)

首先，我们根据上一节介绍的算法，用样本的特征 $\vec{x}$ 计算出神经网络中每个隐藏层节点的输出 $a_i$，以及输出层每个节点的输出 $y_i$。

然后，我们按照下面的方法计算每个节点的误差项 $\delta_i$：

- 对于输出层节点 $i$：
  $$
  \delta_i = y_i(1 - y_i)(t_i - y_i) \tag{3.6}
  $$

  其中，$\delta_i$ 是节点 $i$ 的误差项，$y_i$ 是节点 $i$ 的输出值，$t_i$ 是样本对应于节点 $i$ 的目标值。

  举个例子，对于输出层节点(8)来说，它的输出值是 $y_1$，而样本的目标值是 $t_1$，带入上面的公式得到节点8的误差项 $\delta_8$ 应该是：
  $$
  \delta_8 = y_1(1 - y_1)(t_1 - y_1)
  $$

  

- 对于隐藏层节点：
  $$
  \delta_i = a_i(1 - a_i) \sum_{k \in \text{outputs}} w_{ki} \delta_k \tag{3.7}
  $$

  其中，$a_i$ 是节点 $i$ 的输出值，$w_{ki}$ 是节点 $i$ 到它的下一层节点 $k$ 的连接的权重，$\delta_k$ 是节点 $i$ 的下一层节点 $k$ 的误差项。

  例如，对于隐藏层节点4来说，计算方法如下：
  $$
  \delta_4 = a_4(1 - a_4)(w_{84}\delta_8 + w_{94}\delta_9)
  $$


#### 反向传播算法(Back Propagation)

当所有节点的误差项计算完毕后，我们就可以根据式(3.8) 来更新所有的权重：
$$
w_{ji} \leftarrow w_{ji} + \eta \delta_j x_{ji} \tag{3.8}
$$

其中，$w_{ji}$ 是节点 $i$ 到节点 $j$ 的权重，$\eta$ 是一个成为学习速率的常数，$\delta_j$ 是节点 $j$ 的误差项，$x_{ji}$ 是节点 $i$ 传递给节点 $j$ 的输入。

例如，权重 $w_{84}$ 的更新方法如下：
$$
w_{84} \leftarrow w_{8} + \eta \delta_8 a_4
$$
类似的，权重 $w_{41}$ 的更新方法如下：

$$
w_{41} \leftarrow w_{41} + \eta \delta_4 x_1
$$

偏置项的输入值永远为1。例如，节点4的偏置项 $w_{4b}$ 应该按照下面的方法计算：

$$
w_{4b} \leftarrow w_{4b} + \eta \delta_4
$$

计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是**反向传播算法**的名字的含义。

### 反向传播算法的推导

反向传播的核心思想是通过计算误差对每个权重的影响来调整这些权重，从而减少损失。这个过程实际上是在进行梯度下降，通过逐步调整每个参数（权重）来减小损失。

先确定神经网络的目标函数，取网络所有输出层节点的误差平方和作为目标函数。其中，$E_d$表示是样本的$d$误差。
$$
E_d \equiv \frac{1}{2} \sum_{i \in \text{outputs}} (t_i - y_i)^2 \tag{3.9}
$$
然后，用**随机梯度下降**算法对目标函数进行优化：
$$
w_{ji} \leftarrow w_{ji} - \eta \frac{\partial E_d}{\partial w_{ji}} \tag{3.10}
$$
其中，需求出误差$E_d$对于每个权重$w_{ji}$的偏导数（也就是梯度）。

![img](./assets/2256672-bfbb364740f898d1.webp)

观察上图，我们发现权重 $w_{ji}$ 仅能通过影响节点 $j$ 的输入值影响网络的其它部分。

设 $net_j$ 是节点 $j$ 的加权输入，即
$$
\begin{align*}
net_j &= \overrightarrow{w_j} \cdot \overrightarrow{x_j} \\
&= \sum_i w_{ji} x_{ji}
\end{align*}
\tag{3.11}
$$

$E_d$ 是 $net_j$ 的函数，而 $net_j$ 是 $w_{ji}$ 的函数。根据链式求导法则，可以得到：

$$
\begin{align*}
\frac{\partial E_d}{\partial w_{ji}} &= \frac{\partial E_d}{\partial net_j} \cdot \frac{\partial net_j}{\partial w_{ji}} \\
&= \frac{\partial E_d}{\partial net_j} \cdot x_{ji}
\end{align*}
\tag{3.12}
$$

上式中，$x_{ji}$ 是节点 $i$ 传递给节点 $j$ 的输入值，也就是节点 $i$ 的输出值。

误差项$\delta_j$ 是通过计算每个节点的误差，传播到整个网络的结果。因此，$\delta_j$ 反映了节点 $j$ 对整个损失的贡献，反映了节点 $j$ 的输出与损失函数的敏感度。按梯度下降所需，一个节点的误差项 $\delta$ 是网络误差对这个节点输入的偏导数的相反数（即函数下降最快的方向）：
$$
\delta_j = -\frac{\partial E_d}{\partial net_j}
\tag{3.13}
$$
结合公式(3.10)、公式(3.12)和公式(3.13)可得权重更新算法，即公式(3.8)：
$$
w_{ji} \leftarrow w_{ji} + \eta \delta_j x_{ji}
$$
对于 $\frac{\partial E_d}{\partial net_j}$ 的推导，需要区分输出层和隐藏层两种情况。

#### 输出层权值训练

对于输出层来说，节点$j$的加权输入$net_j$ 仅能通过节点 $j$ 的输出值 $y_j$ 来影响网络其它部分。也就是说 $E_d$ 是 $y_j$ 的函数，而 $y_j$ 是 $net_j$ 的函数，其中 $y_j = \text{sigmoid}(net_j)$。所以我们可以再次使用链式求导法则：
$$
\frac{\partial E_d}{\partial net_j} = \frac{\partial E_d}{\partial y_j} \frac{\partial y_j}{\partial net_j}
$$

根据公式(3.9)，可解得上式第一项：

$$
\begin{align*}
\frac{\partial E_d}{\partial y_j} &= \frac{\partial}{\partial y_j} \frac{1}{2} \sum_{i \in \text{outputs}} (t_i - y_i)^2 \\
&= \frac{\partial}{\partial y_j} \frac{1}{2} (t_j - y_j)^2 \\
&= -(t_j - y_j)
\end{align*}
$$

根据公式(3.3)，可解得上式第一项：

$$
\begin{align*}
\frac{\partial y_j}{\partial net_j} &= \frac{\partial \text{sigmoid}(net_j)}{\partial net_j} \\
&= y_j(1 - y_j)
\end{align*}
$$

将第一项和第二项带入，得到：

$$
\frac{\partial E_d}{\partial net_j} = -(t_j - y_j) y_j(1 - y_j)
$$

令 $\delta_j = -\frac{\partial E_d}{\partial net_j}$，带入上式，即可得到误差计算中的公式(3.6)：

$$
\delta_j = (t_j - y_j) y_j(1 - y_j)
$$

#### 隐藏层权值训练

首先，我们需要定义节点 $j$ 的所有直接下游节点的集合 $Downstream(j)$。下游节点是指那些在节点$j$之后，直接受$net_j$影响的节点（即$j$的输出会作为它们的输入）。例如，对于节点4来说，它的直接下游节点是节点8、节点9。

可以看到 $net_j$ 只能通过影响 $Downstream(j)$ 再影响 $E_d$。设 $net_k$ 是节点 $j$ 的下游节点的输入，则 $E_d$ 是 $net_k$ 的函数，而 $net_k$ 是 $net_j$ 的函数。

因为 $net_k$ 有多个，我们应用全导数公式，可以做出如下推导：
$$
\frac{\partial E_d}{\partial net_j} = \sum_{k \in Downstream(j)} \frac{\partial E_d}{\partial net_k} \frac{\partial net_k}{\partial net_j} \\
$$

这里出现了$\frac{\partial E_d}{\partial net_k}$，这刚好是节点 $k$ 的误差项 $\delta_k$，要另外单独分情况运算。
$$
\frac{\partial E_d}{\partial net_j} = \sum_{k \in Downstream(j)} -\delta_k \frac{\partial net_k}{\partial net_j}
$$
$net_k = \sum_j w_{kj} a_{j} + b_k$，所以$net_k$ 是关于他的输入$a_j$的函数。$a_j = \sigma(net_j)$，所以$a_j$ 又是关于 $net_j$ 的函数。运用链式法则求导：
$$
\begin{align*}
\frac{\partial E_d}{\partial net_j} & = \sum_{k \in Downstream(j)} -\delta_k \frac{\partial net_k}{\partial net_j} \\

& = \sum_{k \in Downstream(j)} -\delta_k \frac{\partial net_k}{\partial a_j} \frac{\partial a_j}{\partial net_j}
\end{align*}
$$
$net_k$ 对 $a_j$ 求偏导，只有第 j 项不为 0，其余项视为常数，求导后为0，得$\frac{\partial net_k}{\partial a_j} = w_{kj}$。结合公式(3.3)，可得sigmoid函数的求导方法。
$$
\begin{align*}
\frac{\partial E_d}{\partial net_j} & = \sum_{k \in Downstream(j)} -\delta_k w_{kj} \frac{\partial a_j}{\partial net_j} \\

& = \sum_{k \in Downstream(j)} -\delta_k w_{kj} a_j(1 - a_j) \\

& = -a_j(1 - a_j) \sum_{k \in Downstream(j)} \delta_k w_{kj} \quad (40)
\end{align*}
$$
因为 $\delta_j = - \frac{\partial E_d}{\partial net_j}$，带入上式得到公式(3.7)：
$$
\delta_j = a_j(1 - a_j) \sum_{k \in Downstream(j)} \delta_k w_{kj}
$$
至此，我们已经推导出了反向传播算法。

### 神经网络的实现

采用面向对象设计，先做一个基本的模型：

<img src="http://upload-images.jianshu.io/upload_images/2256672-2fbae2ee722fbef9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/360" alt="img" style="zoom: 67%;" />

- *Network* 神经网络对象，提供API接口。它由若干层对象组成以及连接对象组成。
- *Layer* 层对象，由多个节点组成。
- *Node* 节点对象计算和记录节点自身的信息(比如输出值、误差项等)，以及与这个节点相关的上下游的连接。
- *Connection* 每个连接对象都要记录该连接的权重。
- *Connections* 仅仅作为Connection的集合对象，提供一些集合操作。

#### Node实现



#### ConstNode对象



#### Layer对象



#### Connection对象



#### Connections对象



#### Network对象
